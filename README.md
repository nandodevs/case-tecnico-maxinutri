# üìä Desafio T√©cnico Maxinutri - Engenheiro de Dados

## üìã Vis√£o Geral

Solu√ß√£o completa de ETL para processamento de dados de e-commerce, implementando um Data Warehouse dimensional com PostgreSQL e orquestra√ß√£o via Apache Airflow. Este projeto processa dados de uma API de e-commerce, realiza transforma√ß√µes e carrega em um modelo dimensional otimizado para an√°lise.

## üèóÔ∏è Arquitetura do Sistema
A arquitetura √© composta por:
1. **Fonte de Dados**: API RESTful paginada com dados de clientes, produtos e pedidos.
2. **Data Lake**: Armazenamento bruto dos dados extra√≠dos em formatos Parquet e CSV.
3. **ETL Pipeline**: Orquestrado pelo Apache Airflow, dividido em tr√™s
    etapas principais:
    - **Extra√ß√£o**: Coleta de dados da API com tratamento de pagina√ß√£o e erros.
    - **Transforma√ß√£o**: Limpeza, valida√ß√£o e enriquecimento dos dados.
    - **Carga**: Inser√ß√£o eficiente no Data Warehouse PostgreSQL.
4. **Data Warehouse**: Modelo dimensional em PostgreSQL com tabelas de fato e dimens√£o.
5. **Monitoramento**: Logs estruturados e sistema de alertas por email para falhas e m√©tricas de performance.

<img src="./docs/imgs/fluxo_etl.png" alt="Fluxo ETL"/>

## üõ†Ô∏è Tecnologias Utilizadas

- **Linguagem**: Python 3.9+
- **Banco de Dados**: PostgreSQL 13+
- **Orquestra√ß√£o**: Apache Airflow 2.5+
- **Formato de Dados**: Parquet, CSV
- **Ferramentas**: Docker, Pandas, Psycopg2, Requests
- **Monitoramento**: Logging estruturado, Alertas por email

## üìä Modelagem Dimensional

### Schema Estrela Implementado

**Tabelas de Dimens√£o:**
- `dim_cliente` - Informa√ß√µes dos clientes (cidade, estado, CEP)
- `dim_produto` - Detalhes dos produtos (categoria, dimens√µes, peso)  
- `dim_tempo` - Dimens√£o temporal (data, ano, m√™s, dia, trimestre)
- `dim_avaliacao` - Avalia√ß√µes e reviews (score, coment√°rios)

**Tabela de Fato:**
- `fato_pedido` - M√©tricas e fatos dos pedidos (pre√ßo, frete, status, datas)

<img src="./docs/imgs/schema_visual_db.png" alt="Modelo Dimensional" width="800"/>

### ü§î Por que Modelo Estrela?

Escolhi o modelo estrela porque:
1. **Simplicidade**: Mais f√°cil de entender e consultar para usu√°rios de neg√≥cio
2. **Performance**: Menos joins necess√°rios para queries anal√≠ticas
3. **Manuten√ß√£o**: Mais simples de manter e evoluir
4. **Escalabilidade**: Adequado para crescimento gradual de dados
5. **Compatibilidade**: Melhor integra√ß√£o com ferramentas BI


## ‚öôÔ∏è Configura√ß√£o e Instala√ß√£o

### Pr√©-requisitos

```bash
# Clone o reposit√≥rio
git clone https://github.com/nandodevs/desafio-engenheiro-dados.git
cd desafio-engenheiro-dados

# Instalar depend√™ncias
pip install -r requirements.txt

# Configurar vari√°veis de ambiente
cp .env.example .env
```

### Configura√ß√£o do Ambiente

Edite o arquivo `.env` com suas configura√ß√µes:

```env
# API Configuration
API_URL=https://teste-engenheiro.onrender.com
TOKEN="sua chave token"

# Adicione as configura√ß√µes do banco Postgres no airflow_settings.yaml
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=desafio_db
POSTGRES_USER=airflow
POSTGRES_PASSWORD=airflow

# Email Alerts (opcional)
EMAIL_HOST=smtp.gmail.com
EMAIL_PORT=587
EMAIL_USER=seu-email@gmail.com
EMAIL_PASSWORD=sua-senha
```
Renomeio o arquivo ".env.example para .env" e adicione as vari√°vies necess√°rias:

```env
# Configura√ß√µes da API
TOKEN = "token da api"
API_URL = "https://teste-engenheiro.onrender.com/data"

# Configura√ß√µes do Airflow
AIRFLOW__CORE__LOAD_EXAMPLES=False
AIRFLOW__WEBSERVER__EXPOSE_CONFIG=True

# Configura√ß√µes de Email (opcional)
SMTP_HOST=smtp.gmail.com
SMTP_PORT=587
SMTP_USER=seuemail@gmail.com
SMTP_PASSWORD=codigoapp

# Email que enviar√° os alertas (pode ser o mesmo ou diferente)
SMTP_MAIL_FROM=seuemail@gmail.com

# Lista de emails que receber√£o os alertas (separados por v√≠rgula)
ALERT_RECIPIENTS=email-destinatario@gmail.com

# Email espec√≠fico para falhas cr√≠ticas
CRITICAL_ALERTS=email-destinatario@gmail.com
```


Instale o Astro CLI no seu computador (Windows):

```bash
winget install -e --id Astronomer.Astro
```

- Execute os comandos abaixo dentro da pasta do projeto:
```bash
# Iniciar a estrutura de pastas e arquivos necess√°rios
astro dev init

#Criar os container Docker e abre a navegador com o Airflow
astro dev start
```

## üîß Estrutura do Projeto

```
projeto-desafio/
‚îú‚îÄ‚îÄ dags/                    # DAGs do Airflow
‚îÇ   ‚îî‚îÄ‚îÄ etl_dag.py          # DAG principal do ETL
‚îú‚îÄ‚îÄ etl/                     # Scripts ETL
‚îÇ   ‚îú‚îÄ‚îÄ extract.py          # Extra√ß√£o de dados da API
‚îÇ   ‚îú‚îÄ‚îÄ transform.py        # Transforma√ß√£o e limpeza
‚îÇ   ‚îú‚îÄ‚îÄ load.py             # Carga no PostgreSQL
‚îÇ   ‚îî‚îÄ‚îÄ monitoring.py       # Sistema de alertas
‚îú‚îÄ‚îÄ sql/                     # Schema e queries
‚îÇ   ‚îî‚îÄ‚îÄ schema.sql          # Schema completo do DW
‚îú‚îÄ‚îÄ data/                    # Camada do data lake
‚îÇ   ‚îú‚îÄ‚îÄ raw/                # Dados brutos da API
‚îÇ   ‚îî‚îÄ‚îÄ processed/          # Dados tratados
‚îú‚îÄ‚îÄ tests/                   # Testes automatizados
‚îÇ   ‚îú‚îÄ‚îÄ test_extract.py
‚îÇ   ‚îú‚îÄ‚îÄ test_transform.py
‚îÇ   ‚îî‚îÄ‚îÄ test_load.py  
‚îú‚îÄ‚îÄ scripts/                 # Scripts auxiliares
‚îÇ   ‚îî‚îÄ‚îÄ validate_new_data.py # Valida√ß√£o de novos dados
‚îî‚îÄ‚îÄ docs/                    # Documenta√ß√£o
    ‚îî‚îÄ‚îÄ architecture.md     # Diagramas de arquitetura
    Dockerfile              # Configura√ß√µes Docker
    docker-compose.yml       
```

## üéØ Funcionalidades Implementadas

### ‚úÖ Extra√ß√£o Resiliente
- Pagina√ß√£o autom√°tica da API com retry mechanism
- Timeout configur√°vel e backoff exponencial
- Salvamento em m√∫ltiplos formatos (Parquet + CSV)
- Tratamento de erros com retry (5 tentativas)

### ‚úÖ Transforma√ß√£o Robusta
- Infer√™ncia autom√°tica de tipos de dados
- Limpeza abrangente e valida√ß√£o de qualidade
- Otimiza√ß√£o de mem√≥ria com tipos espec√≠ficos
- Relat√≥rio detalhado de qualidade dos dados

### ‚úÖ Carga Eficiente
- Inser√ß√£o em lote com controle transacional
- Upsert inteligente com conflic handling
- Logging detalhado de performance
- Valida√ß√£o de integridade referencial

### ‚úÖ Monitoramento e Alertas
- Logs estruturados com diferentes n√≠veis
- Sistema de alertas por email configur√°vel
- M√©tricas de performance do ETL
- Valida√ß√£o p√≥s-carga autom√°tica

## üß™ Valida√ß√£o de Dados:

### Valida√ß√£o de Dados:
- Verifica√ß√£o de integridade referencial entre dimens√µes e fatos
- Valida√ß√£o de ranges para valores num√©ricos
- Consist√™ncia temporal e validade de datas
- Completeness check para campos obrigat√≥rios

## üîÆ Pr√≥ximas Melhorias
### Longo-prazo (3-6 meses):
- [ ] Streaming pipeline com Kafka/Spark Streaming
- [ ] Migra√ß√£o para cloud (BigQuery/Snowflake + Airflow Cloud)
- [ ] Real-time dashboards com Metabase/Grafana
- [ ] Integra√ß√£o com modelos de Machine Learning
- [ ] Sistema de data quality monitoring cont√≠nuo

## Descri√ß√£o detalhada das etapas do ETL

### Extract
- Consome a API paginada (`/data?token=...&page=N`).
- Implementa retries exponenciais com `tenacity` (par√¢metros: up to 5 tentativas, backoff exponencial com m√°ximo de 60s).
- Persiste cada p√°gina como `data/raw/page_N.json` e grava `_meta.json` com informa√ß√£o de pagina√ß√£o e contagens.
- Em caso de erro irrevers√≠vel (status n√£o 200 ap√≥s retries), a task falha e o erro √© logado com stacktrace.

### Explore / Profiling
- Consolida todos os objetos `dados` das p√°ginas em um DataFrame (pandas).
- Gera `reports/profile.md` contendo: linhas, colunas, nulos por coluna, distinct, e amostras de valores.
- Escreve `data/processed/records.csv` para uso da etapa de transforma√ß√£o.

### Transform
- L√™ `data/processed/records.csv` (ou consolida direto dos JSONs se CSV n√£o existir).
- Converte colunas de data (`*_timestamp`, `*_at`, `date*`) para datetime (`pandas.to_datetime` com `errors='coerce'`).
- Converte colunas num√©ricas (ex.: `price`, `freight_value`, `product_weight_g`, dimens√µes do produto) para num√©rico com `pd.to_numeric(..., errors='coerce')`.
- Executa valida√ß√µes sobre os dados (antes do load):
  - Checagem de valores nulos em colunas cr√≠ticas (`order_id`, `customer_id`, `order_purchase_timestamp`).
  - Checagem de duplicados em `order_id` (ou chave candidata definida).
  - Checagem de datas inv√°lidas (linhas com `NaT` ap√≥s parsing).
- Se houver problemas de valida√ß√£o, a etapa levanta `ValueError` e salva `reports/validation_report.txt` com detalhes. O pipeline encerra nesse ponto (evita ingest√£o de dados sujos).

### Load (PostgreSQL)
- Conecta ao Postgres usando SQLAlchemy/psycopg2.
- Cria schema/tabelas (se necess√°rio) e carrega os dados validados em `fato_pedido` (nomeada conforme o case) com `df.to_sql(..., if_exists='replace' ou 'append')` ‚Äî a estrat√©gia usada no projeto pode ser configurada (full replace para entrega, incremental em produ√ß√£o).
- Opcional: cria/atualiza `dim_tempo` a partir da primeira coluna de data encontrada (upsert simples).

---

## Tratamento de erros & Logs
- Retries/backoff: extra√ß√£o usa `tenacity` para proteger contra falhas tempor√°rias da API. Logs mostram tentativas e backoff aplicados.
- Valida√ß√µes expl√≠citas: transform valida e interrompe o pipeline com relat√≥rio em `reports/validation_report.txt` sempre que encontra problemas cr√≠ticos.
- Logs: recomenda-se usar `logging` do Python com configura√ß√£o por vari√°vel `LOG_LEVEL`. Logs por padr√£o podem ser configurados para gravar em `logs/etl.log`. No Airflow, cada task possui sua pr√≥pria sa√≠da de log no UI e discos montados.
- Erro no Airflow: qualquer exce√ß√£o n√£o capturada faz a task falhar e o Airflow registra o traceback completo. Use retries do Airflow (configur√°veis no DAG) para falhas transit√≥rias tamb√©m.

---

## Schema do Data Warehouse (exemplo baseado no projeto)
A imagem do banco est√° em `docs/imgs/schema_visual_db.png` (fornecida) e o diagrama ETL est√° em `docs/imgs/etl_flow.png` (gerado). Tabelas principais esperadas no DW:

- dim_cliente (cliente_id, nome, cidade, estado, zip_prefix, ...)
- dim_produto (product_id, nome_categoria, peso_g, comprimento_cm, altura_cm, largura_cm, ...)
- dim_avaliacao (review_id, review_score, comment, created_at, answered_at, ...)
- dim_tempo (date_key, year, month, day, weekday)
- fato_pedido (order_id, customer_id -> dim_cliente, product_id -> dim_produto, order_status, price, freight_value, order_purchase_timestamp -> dim_tempo_key, ...)

Exemplo simples de cria√ß√£o (em `sql/schema.sql`):
```sql
CREATE TABLE IF NOT EXISTS public.dim_cliente (
  customer_id TEXT PRIMARY KEY,
  customer_city TEXT,
  customer_state TEXT,
  customer_zip_code_prefix INT
);

CREATE TABLE IF NOT EXISTS public.dim_produto (
  product_id TEXT PRIMARY KEY,
  product_category_name TEXT,
  product_weight_g INT,
  product_length_cm INT,
  product_height_cm INT,
  product_width_cm INT
);

CREATE TABLE IF NOT EXISTS public.dim_tempo (
  date_key DATE PRIMARY KEY,
  year INT,
  month INT,
  day INT,
  weekday INT
);

CREATE TABLE IF NOT EXISTS public.fato_pedido (
  order_id TEXT PRIMARY KEY,
  customer_id TEXT,
  product_id TEXT,
  order_status TEXT,
  order_purchase_timestamp TIMESTAMP,
  price NUMERIC,
  freight_value NUMERIC
);
```

---

## Observabilidade e testes
- Relat√≥rios: `reports/profile.md` (profiling inicial) e `reports/validation_report.txt` (falhas de valida√ß√£o).
- Testes unit√°rios: recomenda-se adicionar testes para as fun√ß√µes de parsing/valida√ß√£o (ex.: `tests/etl/test_transform.py`).
- Checks: contagens por etapa (raw rows vs loaded rows) e checksums podem ser adicionados a uma tabela de metadata para auditoria.

---

## üôã‚Äç‚ôÇÔ∏è FAQ

**Q: Por que n√£o usar Kafka para ingest√£o?**
**R:** Para este volume de dados (API paginada com ~1000 registros), uma solu√ß√£o batch √© mais simples e adequada. Kafka seria overengineering e adicionaria complexidade desnecess√°ria.

**Q: Por que Python e n√£o Spark?**
**R:** Python + Pandas √© suficiente para este volume de dados e mais simples de manter. Spark seria necess√°rio apenas para volumes > 10GB ou processamento streaming.

**Q: Como lidar com dados incrementais?**
**R:** A DAG √© projetada para processamento incremental usando timestamps dos registros. O sistema verifica a √∫ltima data processada e s√≥ busca dados novos.

**Q: Qual a estrat√©gia de tratamento de erros?**
**R:** Implementamos retry com backoff exponencial, logging estruturado, alertas por email e transa√ß√µes at√¥micas para garantir consist√™ncia.

**Q: Como escalar este projeto?**
**R:** A arquitetura permite escalar horizontalmente com: particionamento de tabelas, processamento distribu√≠do com Spark, e migra√ß√£o para cloud.

---

## üéØ Conclus√£o

Esta solu√ß√£o demonstra habilidades completas em engenharia de dados, desde ingest√£o de APIs at√© modelagem dimensional e automa√ß√£o com Airflow. A arquitetura √© robusta, escal√°vel e mant√©m simplicidade onde poss√≠vel, seguindo as melhores pr√°ticas do mercado.

**Diferenciais Implementados:**
- ‚úÖ Documenta√ß√£o completa e detalhada
- ‚úÖ Tratamento de erros robusto com retry mechanism
- ‚úÖ Modelagem dimensional bem fundamentada
- ‚úÖ Automatiza√ß√£o completa com Apache Airflow
- ‚úÖ Otimiza√ß√µes de performance e uso de mem√≥ria
- ‚úÖ Sistema de monitoramento e alertas
- ‚úÖ Testes automatizados e valida√ß√£o de dados
- ‚úÖ Prepara√ß√£o para escalabilidade futura

---

*Este projeto foi desenvolvido como parte do processo seletivo para Engenheiro de Dados. Para d√∫vidas ou mais informa√ß√µes, entre em contato atrav√©s do GitHub.*