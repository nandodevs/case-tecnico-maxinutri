# dags/etl_dag.py
from __future__ import annotations

import pendulum
import sys
from airflow.models.dag import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook
from psycopg2.sql import SQL, Identifier
from pathlib import Path
import logging
from psycopg2 import OperationalError, ProgrammingError, DataError, InterfaceError
from airflow.utils.state import State

# Import do sistema de alertas
try:
    from etl.monitoring import on_failure_callback, on_success_callback, alert_system
except ImportError as e:
    logging.warning(f"Sistema de alertas n√£o dispon√≠vel: {e}")
    # Fallback simples
    def on_failure_callback(context):
        logging.error(f"Falha na tarefa: {context}")
    def on_success_callback(context):
        logging.info(f"Tarefa conclu√≠da: {context}")
    alert_system = None

logger = logging.getLogger(__name__)

# Adiciona o diret√≥rio `etl` ao PYTHONPATH para que os scripts possam ser importados
sys.path.append(str(Path(__file__).parent.parent / "etl"))

# Importa as fun√ß√µes principais dos seus scripts
from extract import main as extract_main
from transform import main as transform_main
from load import main as load_main

# Callbacks para monitoring
def dag_failure_callback(context):
    """Callback para falhas globais da DAG"""
    try:
        dag_id = context['dag'].dag_id
        execution_date = context['execution_date']
        
        error_message = str(context.get('exception', 'Erro desconhecido'))
        
        logger.critical(f"‚ùå FALHA GLOBAL na DAG {dag_id}: {error_message}")
        
        if alert_system:
            alert_system.send_email_alert(
                f"üö® FALHA GLOBAL - DAG {dag_id}",
                f"""Falha global no pipeline ETL:

DAG: {dag_id}
Data: {execution_date}
Erro: {error_message}

Status: Pipeline completamente parado
A√ß√£o: Interven√ß√£o imediata necess√°ria""",
                to_emails=["admin@empresa.com", "team@empresa.com"]
            )
            
    except Exception as e:
        logger.error(f"Erro no callback de falha global: {e}")

def dag_success_callback(context):
    """Callback para sucesso global da DAG"""
    try:
        dag_id = context['dag'].dag_id
        execution_date = context['execution_date']
        
        logger.info(f"‚úÖ DAG {dag_id} conclu√≠da com sucesso")
        
        if alert_system:
            alert_system.send_email_alert(
                f"‚úÖ SUCESSO - DAG {dag_id} Conclu√≠da",
                f"""Pipeline ETL executado com sucesso:

DAG: {dag_id}
Data: {execution_date}
Status: Todos os dados processados com sucesso

Tempo de execu√ß√£o: {context['dag_run'].duration}""",
                to_emails=["team@empresa.com"]
            )
            
    except Exception as e:
        logger.error(f"Erro no callback de sucesso global: {e}")

with DAG(
    dag_id="desafio_etl_maxinutri",
    start_date=pendulum.datetime(2023, 1, 1, tz="UTC"),
    schedule="@daily",
    catchup=False,
    tags=["etl", "desafio"],
    on_failure_callback=dag_failure_callback,
    on_success_callback=dag_success_callback,
    default_args={
        'on_failure_callback': on_failure_callback,
        'on_success_callback': on_success_callback,
        'email_on_failure': False,  # Desativar emails padr√£o do Airflow
        'email_on_retry': False,
        'retries': 2,
        'retry_delay': pendulum.duration(minutes=5),
        'execution_timeout': pendulum.duration(minutes=120),
    }
) as dag:
    
    def run_extract_task():
        """Executa a extra√ß√£o dos dados."""
        try:
            logger.info("Iniciando extra√ß√£o de dados da API...")
            extract_main()
            logger.info("‚úÖ Extra√ß√£o conclu√≠da com sucesso.")
        except Exception as e:
            logger.error(f"‚ùå Erro na extra√ß√£o: {e}")
            raise

    def run_transform_task():
        """Executa a transforma√ß√£o dos dados."""
        try:
            logger.info("Iniciando transforma√ß√£o de dados...")
            transform_main()
            logger.info("‚úÖ Transforma√ß√£o conclu√≠da com sucesso.")
        except Exception as e:
            logger.error(f"‚ùå Erro na transforma√ß√£o: {e}")
            raise

    def run_create_database():
        """Cria o banco de dados se ele n√£o existir."""
        hook = PostgresHook(postgres_conn_id="postgres-default")
        conn = None
        cur = None
        try:
            conn = hook.get_conn()
            conn.autocommit = True
            cur = conn.cursor()
            
            db_name = "desafio_db"

            cur.execute(SQL("SELECT 1 FROM pg_database WHERE datname = %s"), (db_name,))
            exists = cur.fetchone()

            if not exists:
                logger.info(f"Criando o banco de dados '{db_name}'...")
                cur.execute(SQL("CREATE DATABASE {}").format(Identifier(db_name)))
                logger.info(f"‚úÖ Banco de dados '{db_name}' criado com sucesso.")
            else:
                logger.info(f"üìä Banco de dados '{db_name}' j√° existe.")

        except OperationalError as e:
            logger.error(f"‚ùå Erro de conex√£o ao criar banco: {e}")
            raise
        except ProgrammingError as e:
            logger.error(f"‚ùå Erro de SQL ao criar banco: {e}")
            raise
        except Exception as e:
            logger.error(f"‚ùå Erro inesperado ao criar banco: {e}")
            raise
        finally:
            if cur:
                cur.close()
            if conn:
                conn.close()
    
    def run_load_task():
        """Cria a conex√£o e executa o carregamento dos dados no banco de dados."""
        conn = None
        cur = None
        try:
            hook = PostgresHook(postgres_conn_id="postgres-default")
            conn = hook.get_conn()
            conn.autocommit = False
            
            # Configura timeout para evitar opera√ß√µes muito longas
            cur = conn.cursor()
            cur.execute("SET statement_timeout = 300000;")  # 5 minutos
            
            logger.info("Iniciando carga de dados...")
            load_main(cur=cur)
            conn.commit()
            logger.info("‚úÖ Dados carregados com sucesso.")
            
        except (OperationalError, InterfaceError) as e:
            logger.error(f"‚ùå Erro de conex√£o durante carga: {e}")
            if conn:
                conn.rollback()
            raise
        except DataError as e:
            logger.error(f"‚ùå Erro de dados durante carga: {e}")
            if conn:
                conn.rollback()
            raise
        except Exception as e:
            logger.error(f"‚ùå Erro inesperado durante carga: {e}")
            if conn:
                conn.rollback()
            raise
        finally:
            if cur:
                cur.close()
            if conn:
                conn.close()

    def validate_etl_process():
        """Valida√ß√£o simplificada do ETL."""
        hook = PostgresHook(postgres_conn_id="postgres-default", schema="desafio_db")
        conn = None
        cur = None
        try:
            conn = hook.get_conn()
            cur = conn.cursor()
            
            # Lista de verifica√ß√µes a serem realizadas
            checks = [
                ("Tabelas existem", """
                    SELECT COUNT(*) 
                    FROM information_schema.tables 
                    WHERE table_schema = 'public'
                    AND table_name IN ('dim_cliente', 'dim_produto', 'dim_tempo', 'dim_avaliacao', 'fato_pedido')
                """),
                ("Total de clientes", "SELECT COUNT(*) FROM dim_cliente"),
                ("Total de produtos", "SELECT COUNT(*) FROM dim_produto"),
                ("Total de pedidos", "SELECT COUNT(*) FROM fato_pedido"),
                ("Pedidos v√°lidos", "SELECT COUNT(*) FROM fato_pedido WHERE preco > 0 AND frete >= 0")
            ]
            
            results = {}
            
            for check_name, query in checks:
                try:
                    cur.execute(query)
                    result = cur.fetchone()[0]
                    results[check_name] = result
                    logger.info(f"{check_name}: {result}")
                except Exception as e:
                    logger.warning(f"Falha na verifica√ß√£o '{check_name}': {e}")
                    results[check_name] = f"Erro: {e}"
            
            # Verificar se todas as tabelas essenciais existem
            if results.get("Tabelas existem", 0) < 5:
                raise Exception("N√£o todas as tabelas foram criadas corretamente")
            
            # Verificar se h√° dados nas tabelas principais
            if results.get("Total de pedidos", 0) == 0:
                logger.warning("‚ö†Ô∏è  Tabela de pedidos est√° vazia")
            
            if results.get("Total de clientes", 0) == 0:
                logger.warning("‚ö†Ô∏è  Tabela de clientes est√° vazia")
            
            if results.get("Total de produtos", 0) == 0:
                logger.warning("‚ö†Ô∏è  Tabela de produtos est√° vazia")
            
            logger.info("‚úÖ Valida√ß√£o conclu√≠da com sucesso")
            logger.info(f"üìä Resultados: {results}")
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Falha na valida√ß√£o: {e}")
            raise
        finally:
            if cur:
                cur.close()
            if conn:
                conn.close()

    def send_final_report():
        """Envia relat√≥rio final do processamento."""
        try:
            if alert_system:
                alert_system.send_email_alert(
                    "üìä Relat√≥rio Di√°rio - ETL Conclu√≠do",
                    """Processamento ETL di√°rio conclu√≠do com sucesso!

‚úÖ Extra√ß√£o: Dados extra√≠dos da API
‚úÖ Transforma√ß√£o: Dados limpos e processados
‚úÖ Carga: Dados carregados no Data Warehouse
‚úÖ Valida√ß√£o: Qualidade dos dados verificada

Status: Pipeline completo executado com sucesso""",
                    to_emails=["team@empresa.com", "management@empresa.com"]
                )
                logger.info("‚úÖ Relat√≥rio final enviado")
            else:
                logger.info("üìä Processamento conclu√≠do (relat√≥rio n√£o enviado)")
                
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Falha ao enviar relat√≥rio: {e}")

    # Definindo as tarefas
    extract_task = PythonOperator(
        task_id="extract_data_from_api",
        python_callable=run_extract_task,
        retries=2,
        retry_delay=pendulum.duration(seconds=30),
        execution_timeout=pendulum.duration(minutes=30),
    )

    transform_task = PythonOperator(
        task_id="transform_data",
        python_callable=run_transform_task,
        retries=2,
        retry_delay=pendulum.duration(seconds=30),
        execution_timeout=pendulum.duration(minutes=30),
    )

    create_db_task = PythonOperator(
        task_id="create_database_if_not_exists",
        python_callable=run_create_database,
        retries=2,
        retry_delay=pendulum.duration(seconds=30),
        execution_timeout=pendulum.duration(minutes=5),
    )

    load_task = PythonOperator(
        task_id="load_data_to_postgres",
        python_callable=run_load_task,
        retries=3,
        retry_delay=pendulum.duration(seconds=60),
        execution_timeout=pendulum.duration(minutes=60),
    )

    validate_task = PythonOperator(
        task_id="validate_etl_process",
        python_callable=validate_etl_process,
        retries=1,
        retry_delay=pendulum.duration(seconds=30),
        execution_timeout=pendulum.duration(minutes=10),
    )

    report_task = PythonOperator(
        task_id="send_final_report",
        python_callable=send_final_report,
        retries=1,
        retry_delay=pendulum.duration(seconds=15),
        execution_timeout=pendulum.duration(minutes=5),
    )

    # Definindo as depend√™ncias
    extract_task >> transform_task >> create_db_task >> load_task >> validate_task >> report_task